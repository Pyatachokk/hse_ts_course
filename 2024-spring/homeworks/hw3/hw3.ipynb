{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Введение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом разделе нам необходимо построить прогнозные модели для курсов валют. На семинарах мы обсуждали, что прогнозировать цены котировок стандартными линейными моделями довольно бесперспективно. Так как такие данные близки к модели случайного блуждания, оптимальным прогнозом для них часто оказывается наивный. Однако если перейти к более низкой частоте (например, к месячным данным), то некоторого превосходства над наивной всё же можно добиться.\n",
    "\n",
    "В приложенном датасете currencies.csv находятся следующие величины:\n",
    "1.  Курсы ряда валют по отношению к доллару. \n",
    "\n",
    "    Курс EUR/USD - Евро Доллар США\n",
    "    \n",
    "    Курс CNY/USD - Китайский юань Доллар США\n",
    "    \n",
    "    Курс INR/USD - Индийская рупия Доллар США\n",
    "    \n",
    "    Курс JPY/USD - Японская йена Доллар США\n",
    "    \n",
    "    Курс GBR/USD - Британский фунт Доллар США\n",
    "    \n",
    "    Курс CHF/USD - Швейцарский франк Доллар США\n",
    "    \n",
    "    Курс BRL/USD - Бразильский реал Доллар США\n",
    "    \n",
    "    Курс IDR/USD - Индонезийская рупия Доллар США\n",
    "\n",
    "2. Цена нефти BRENT за баррель и цена газа в Европе за mmbtu (Британская тепловая единица)\n",
    "3. ff_rate -- ставка ФРС США\n",
    "\n",
    "Нашей задачей будет построить прогнозную модель для всех курсов валют из пункта 1. Цены нефти, газа и ставка ФРС даны дополнительно, можете использовать их если посчитаете необходимым, но в базовой версии можно использовать только временные ряды курсов валют."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт данных\n",
    "\n",
    "Импортируйте данные из файла. Приведите информацию о датах в один столбец типа datetime. Например, чтобы каждая точка отображала дату начала месяца. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Писать для каждого ряда одномерную модель несколько расточительно, хотя и не лишено смысла. Этот процесс необходимо автоматизировать. В простом для понимания варианте можно свести прогноз каждой валюты к табличной задаче и подобрать регрессоры из остальных рядов. Мы таким уже занимались, поэтому попробуем зайти с немного другой стороны.\n",
    "\n",
    "Весь рынок и в частности рынок валют -- единый механизм, находящийся в некотором равновесии и иногда от него отклоняющийся. Значит, у рядов должен быть общий тренд. Формально такая ситуация называется коинтеграцией и это тема отдельной лекции, нам она будет нужна только для идеи. Более подробно про коинтеграцию можно почитать вот в этом [конспекте](https://vk.com/doc126754362_567660819?hash=AvDGHaO92KX7exjBCleLZsEGHPPX2iZfCqae2Fijkng). \n",
    "\n",
    "### 1. (0.5 балла) Визуализация\n",
    "\n",
    "Давайте увидим это явно. Возьмите все курсы валют, отнормируйте их c помощью StandardScaler из sklearn и изобразите результат на одном графике. \n",
    "\n",
    "График будет немного шумный, но вы должны заметить что в целом валюты движутся по схожим траекториям, а некоторые группируются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно ли выделить эти общие тренды на рынке? Да! Можно просто рассмотреть все валюты как один многомерный вектор и понизить его размерность.\n",
    "\n",
    "Конечно, для временных рядов существуют специфические методы понижения размерности, но они выходят за рамки нашего курса. Поэтому мы воспользуемся не слишком подходящим, но зато простым и знакомым методом главных компонент (PCA). PCA никак не учитывает временную зависимость точек, но мы позволим себе пренебречь этим.\n",
    "\n",
    "### 2. (0.5 балла) Визуализация главных компонент\n",
    "\n",
    "Примените PCA на рядах из восьми валют и отберите три первые главные компоненты. Изобразите их на одном графике. Сравните с предыдущим графиком. Компоненты будут примерно похожи на тренды нескольких разных групп валют."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. VAR для каждой валюты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хочется построить какую-то общую модель, которая учтёт эти тренды на рынке и позволит с её помощью прогнозирвать отдельные валюты. Мы проходили только VAR(p) из прогнозирующих моделей, но её достаточно в большинстве случаев. Также мы обсуждали, что количество параметров в VAR(p)-модели равно \n",
    "\n",
    "$$n^2 p + n + \\frac{n(n+1)}{2},$$\n",
    "\n",
    "где n число уравнений системы. Данное выражение растёт квадратично по n. Значит, количество уравнений нам бы хотелось минимизировать. У нас в наличии 8 валют и ещё потенциально 3 дополнительных переменных. Это слишком много для стандартной VAR-модели, необходимо сократить размерность.\n",
    "\n",
    "Будем для каждой валюты строить VAR(p) модель со следующими рядами: ряд этой валюты + из всех остальных рядов выделим PCA-компоненты. Дальнейшие шаги необходимо будет сделать для каждой из валют в списке, поэтому рекомендуется заранее писать код так, чтобы его было легко встроить в цикл. Например, разбивать код на функции и т.п. В качестве референса предлагается заполнить класс PartialForecaster. Это будет класс для прогнозирвоания одной валюты на основе всех остальных. Ваша реализация может отличаться, меняйте параметры или добавляйте свои методы в класс если необходимо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (0.5 балла) Реализуйте метод fit_pca, который вычисляет главные компоненты от фичей и сохраняет их. Компоненты должны покрывать 95% дисперсии оригинальных данных. Не добавьте в вычисление PCA целевую фичу, это будет сильная утечка!\n",
    "\n",
    "2. (0.5 балла) Реализуйте метод fit_var. В нём необходимо склеить целевую переменную и полученные главные компоненты в один датасет и обучить на этом VAR(p)-модель.\n",
    "\n",
    "    Важное замечание. По-хорошему нужно оценивать модель на стационарных данных. Если бы мы оценивали модель методом максимального правдоподобия с ограничениями на стационарность, то ничего хорошего бы не вышло, коэффициенты бы сели на границу этого ограничения. Но так как в statsmodels (да и в целом чаще всего) модель оценивается методом наименьших квадратов и без ограничений на параметры, это можно проигнорировать в нашем случае. Для прогнозирования особой разницы не будет, но часть статистических тестов работать перестанет из-за возможных смещений в распределениях оценок параметров.\n",
    "\n",
    "3. (0.5 балла) Реализуйте метод forecast. По сути нужно просто спрогнозировать VAR и достать из прогноза только один ряд для нужной валюты.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "import pandas as pd\n",
    "\n",
    "class PartialForecaster:\n",
    "    \n",
    "    def __init__(self, data:pd.DataFrame, target_feature: str, p: int | None = None):\n",
    "        \"\"\"\n",
    "        __init__ Initializing forecaster class\n",
    "\n",
    "        Arguments:\n",
    "            data {pd.DataFrame} -- Dataframe of all data\n",
    "            target_feature {str} -- Target feature of dataframe. Other features in dataframe will be regressors\n",
    "            p {int} -- Order of VAR model\n",
    "        \"\"\"\n",
    "        self.is_pca_fitted = False\n",
    "        self.is_var_fitted = False\n",
    "\n",
    "        assert target_feature in data.columns\n",
    "\n",
    "        self.data = data \n",
    "        self.target_feature = target_feature\n",
    "        self.p = p\n",
    "\n",
    "        \n",
    "        \n",
    "    def fit_pca(self, covered_variance=0.95) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        fit_pca _summary_\n",
    "\n",
    "        Arguments:\n",
    "            features {pd.DataFrame} -- Dataframe of features\n",
    "\n",
    "        Keyword Arguments:\n",
    "            covered_variance {float} -- Share of variance, covered by PCA from original data. (default: {0.95})\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame -- Dataframe of principal components\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Your code\n",
    "        \n",
    "        self.is_pca_fitted = True\n",
    "\n",
    "    def fit_var(self):\n",
    "\n",
    "        \"\"\"\n",
    "         Fitting var on results of PCA\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.is_pca_fitted\n",
    "\n",
    "        self.model = ... # Your code\n",
    "        \n",
    "\n",
    "        self.is_var_fitted = True\n",
    "\n",
    "\n",
    "    def forecast(self, h: int = 12) -> pd.Series:\n",
    "        \"\"\"\n",
    "        forecast Forecast the entire model from the end of training data h steps forward\n",
    "\n",
    "        Keyword Arguments:\n",
    "            h {int} -- Forecasting horizon (default: {1})\n",
    "\n",
    "        Returns:\n",
    "            pd.Series -- Forecast of target_feature\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.is_pca_fitted and self.is_var_fitted \n",
    "\n",
    "        # Your code\n",
    "\n",
    "        forecast = ...\n",
    "\n",
    "        assert type(forecast) is pd.Series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. (1 балл) Отберите параметр p и посчитайте прогнозную ошибку такого метода для каждой валюты. \n",
    "\n",
    "    Предлагается следующая процедура.\n",
    "\n",
    "    1. Выбираем некоторое стартовое окно. У нас 283 точки. 36 (12*3) последних точек выделим на тестовую часть, остальное на тренировочную.\n",
    "    2. На тренировочной части выбираем p по информационному критерию. В классе выше это означает p = None\n",
    "\n",
    "    Далее попробуем получить наиболее репрезентативную оценку ошибки для такого p. Мы будем сразу считать относительную ошибку в сравнении с наивной моделью, так как на котировках часто нельзя построить статистическую модель лучше наивной. \n",
    "\n",
    "    1. Прогнозируем полученной моделью на 12 шагов вперёд.\n",
    "    2. Считаем абсолютную ошибку прогноза (вектор длины 12)\n",
    "    3. Строим наивный прогноз\n",
    "    4. Считаем абсолютную ошибку наивного прогноза\n",
    "    5. Считаем отношение ошибки нашего прогноза к наивной ошибке, сохраняем этот вектор длины 12\n",
    "    7. Увеличиваем тренировочную выборку на 6 наблюдений. Переоцениваем всю модель на новых данных, но уже при фиксированном p. В классе выше это будет p={некоторое число}\n",
    "\n",
    "    Повторяем эту процедуру пока не закончатся данные. При текущих параметрах получится 4 итерации. Усредняем вектора ошибок по всем итерациям. Итого получаем усреднённый вектор длины 12. Параметры можете поменять по собственным соображениям, но поясните логику.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_score_partial(initial_window_size: int = 247, step: int = 5, horizon: int = 12) -> pd.Series:\n",
    "    \"\"\"\n",
    "    cross_val_score Estimating MAE cross-val score on a fitted model. Needed to get more sustainable MAE estimation\n",
    "\n",
    "    Arguments:\n",
    "        initial_window_size {int} -- Initial size of expanding window\n",
    "        step {int} -- Step size of expanding window\n",
    "        horizon {int} -- Forecasting horizon of cross-validation score\n",
    "        p {int} -- order of VAR model\n",
    "    Returns:\n",
    "        pd.Series -- Vector of absolute error by each horizon, averaged by several folds\n",
    "    \"\"\"\n",
    "\n",
    "    # Your code\n",
    "\n",
    "    score = ...\n",
    "\n",
    "    assert type(score) is pd.Series\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. (0.5 балла) Постройте линейный график скоров. Можно строить любой другой график кроме линейного, если он будет более интерпретируем. Простор для творчества!\n",
    "\n",
    "    1.  Каждая валюта представлена линией на графике\n",
    "    2.  По оси абсцисс идёт горизонт прогнозирования (от 1 до 12)\n",
    "    3.  По оси ординат отношение ошибки VAR-модели к ошибке наивной модели\n",
    "\n",
    "Получилось ли по какой-то валюте стабильно предсказывать лучше наивной на всех горизонтах. По каким валютам получилось лучше, по каким хуже?\n",
    "\n",
    "У нас получалось, что на сильно зарегулированных экономиках (например, Китай) прогнозы близки к наивным, так как курс валюты сильнее зависит от решений партии, а не внешней обстановки. Но в целом у вас может получиться и другой результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Автокодировщик на стероидах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно построить альтернативный вариант.\n",
    "\n",
    "1. Энкодер. Строим PCA на всех валютах (и доп фичах, если хотите) сразу, выделяем главные компоненты.\n",
    "2. Декодер. PCA не обратим в стандартном понимании, нельзя аналитически получить из главных компонент обратно исходные ряды. Для каждой валюты обучаем регрессионную модель, предсказывающую курс валюты по главным компонентам. Если у нас 8 валют, здесь получится 8 моделей. Модели можно взять любые (линрег, бустинг, ...). Не мучайтесь с подбором гиперпараметров, бустинга из коробки хватит.\n",
    "2. Строим VAR(p)-модель только на главных компонентах, отобрав p\n",
    "4. Прогнозируем главные компоненты вперёд\n",
    "5. Применяем модели-декодеры, чтобы получить из прогнозов главных компоненты прогнозы валют\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (0.5 балла) Реализуйте метод fit_pca, который вычисляет главные компоненты от фичей и сохраняет их. Компоненты должны покрывать 95% дисперсии оригинальных данных. В этот раз нужно взять в вычисление все доступные ряды\n",
    "\n",
    "2. (0.5 балла) Реализуйте метод fit_var. В нём необходимо склеить целевую переменную и полученные главные компоненты в один датасет и обучить на этом VAR(p)-модель.\n",
    "\n",
    "3. (1 балл) Реализуйте метод fit_decoders. Необходимо оценить ряд регрессий, восстанавливающих валюты из главных компонент. Восстанавливать дополнительные переменные не нужно. Заморачиваться с отбором параметров этих моделей не нужно (только при большом желании). Бустинга из коробки должно хватить.\n",
    "\n",
    "4. (0.5 балла) Реализуйте метод forecast. VAR прогнозируют главные компоненты, а модели-декодеры восстанавливают из этих прогнозов валюты. Функция должна возвращать датафрейм прогнозов всех валют на 12 шагов. По колонкам идут валюты, по строкам горизонты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "import pandas as pd\n",
    "\n",
    "class EncoderDecoderForecaster:\n",
    "    \n",
    "    def __init__(self, data:pd.DataFrame, target_feature: str, p: int|None = None):\n",
    "        \"\"\"\n",
    "        __init__ Initializing forecaster class\n",
    "\n",
    "        Arguments:\n",
    "            data {pd.DataFrame} -- Dataframe of all data\n",
    "            target_feature {str} -- Target feature of dataframe. Other features in dataframe will be regressors\n",
    "            p {int} -- Order of VAR model\n",
    "        \"\"\"\n",
    "        self.is_pca_fitted = False\n",
    "        self.is_var_fitted = False\n",
    "\n",
    "        assert target_feature in data.columns\n",
    "\n",
    "        self.data = data \n",
    "        self.target_feature = target_feature\n",
    "        self.p = p\n",
    "\n",
    "        self.decoders = []\n",
    "        \n",
    "        \n",
    "    def fit_pca(self, covered_variance=0.95) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        fit_pca Fitting PCA on a bunch of currencies\n",
    "\n",
    "        Arguments:\n",
    "            features {pd.DataFrame} -- Dataframe of features\n",
    "\n",
    "        Keyword Arguments:\n",
    "            covered_variance {float} -- Share of variance, covered by PCA from original data. (default: {0.95})\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame -- Dataframe of principal components\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Your code\n",
    "        \n",
    "        self.is_pca_fitted = True\n",
    "\n",
    "\n",
    "    def fit_var(self):\n",
    "\n",
    "        \"\"\"\n",
    "         Fitting var on results of PCA\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.is_pca_fitted\n",
    "\n",
    "        self.model = ... # Your code\n",
    "        \n",
    "\n",
    "        self.is_var_fitted = True\n",
    "\n",
    "    def fit_decoders(self):\n",
    "\n",
    "        # Your code\n",
    "\n",
    "        assert len(self.decoders) > 0\n",
    "        self.is_decoder_fitted = True\n",
    "\n",
    "\n",
    "    def forecast(self, h: int = 12) -> pd.Series:\n",
    "        \"\"\"\n",
    "        forecast Forecast VAR and decode it's forecasts with decoder-models.\n",
    "\n",
    "        Keyword Arguments:\n",
    "            h {int} -- Forecasting horizon (default: {1})\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame -- Forecasts of all currencies. Horizon by rows. Currencies by columns.\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.is_pca_fitted and self.is_var_fitted and self.is_decoder_fitted\n",
    "\n",
    "        # Your code\n",
    "\n",
    "        forecast = ...\n",
    "\n",
    "        assert type(forecast) is pd.Series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. (1 балл) Аналогично предыдущей части, подсчитайте относительную ошибку по всем валютам. Отличие будет только в том, что в этот раз класс EncoderDecoderForecaster возвращает прогнозы по всем валютам сразу. В данном случае итог будет сразу датафреймом. По колонкам будут валюты, по строкам горизонты. Параметр p отберите на тренировочной выборке и усредните относительную ошибку прогноза (усреднить датафреймы поэлементно) по всем окнам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_cross_val_score_encoder_decoder(self, data: pd.DataFrame, initial_window_size: int, step: int, p: int, horizon: int = 12) -> pd.Series:\n",
    "    \"\"\"\n",
    "    cross_val_score Estimating MAE cross-val score on a fitted model. Needed to get more sustainable MAE estimation\n",
    "\n",
    "    Arguments:\n",
    "        initial_window_size {int} -- Initial size of expanding window\n",
    "        step {int} -- Step size of expanding window\n",
    "        horizon {int} -- Forecasting horizon of cross-validation score\n",
    "        p {int} -- order of VAR model\n",
    "    Returns:\n",
    "        pd.DataFrame -- Frame of absolute error by each horizon and each currency, averaged by several folds\n",
    "    \"\"\"\n",
    "\n",
    "    assert self.model is not None\n",
    "\n",
    "    # Your code\n",
    "\n",
    "    score = ...\n",
    "\n",
    "    assert type(score) is pd.DataFrame\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. (0.5 балла) Постройте график относительных ошибок аналогично предыдущей части. Получилось ли добиться улучшения относительно предыдущей модели? Результат здесь не очевиден, метод может оказаться как лучше, так и хуже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Расширения модели (2 балла)\n",
    "\n",
    "Придумайте способ добавить в PCA (t-sne или любой другой кросс-секционный метод) зависимость от времени. Либо найдите метод понижения размерности специально для временных рядов. Кратко опишите суть и примените его для обоих классов выше вместо PCA. Получилось ли улучшить качество модели?\n",
    "\n",
    "1. (1 балл) Ваше краткое описание методики и почему она подходит для рядов. Если есть статьи, можете приложить ссылки.\n",
    "\n",
    "2. (1 балл) Реализация для обеих вариантов, подсчёт метрик и анализ результата."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Рубрика \"как вам домашка?\" (0.1 балла)\n",
    "\n",
    "Пройдите короткий опрос. Это действительно важно. https://forms.gle/w3sV453spERTbGvr7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
