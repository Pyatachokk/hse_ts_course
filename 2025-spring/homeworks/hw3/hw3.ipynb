{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В файле `games.csv` собран датасет по посещаемости 80 популярных игр (да-да, аналогично предыдущей дз). Нам нужен был достаточно гомогенный датасет. Так как посещаемость в первые дни после выхода игры обычно аномальная, рекомендуется внимательно посмотреть на начало рядов и при необходимости отсечь часть стартовых наблюдений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SARIMA (2 балла)\n",
    "\n",
    "Для этого раздела выберите наиболее близкую вам игру, или же возьмите случайную. Во всех рядах возьмём недельную сезонность с периодом 7. Она по структуре данных ожидается наиболее сильной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0.5 балла)\n",
    "Отберите параметр D на основе силы сезонности STL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0.5 балла) Отберите параметр d на основе ADF или KPSS. Убедитесь, что правильно специфицировали тесты. Аргументируйте ваш выбор. Ваша спецификация должна быть согласована со структурой данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0.5 балла) Примените к ряду разности согласно D и d. Изобразите ACF и PACF для стационарного ряда. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0.5) Определите по коррелограммам параметры P,Q,p,q или их границы в случае неоднозначности. Подберите оптимальные параметры по AIC. Длину трейна и горизонт прогнозирования специфицируйе самостоятельно, выбор обоснуйте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Проверка гомогенности (2 балла)\n",
    "\n",
    "Попробуем понять, достаточно ли однороден наш датасет. Будем использовать все ряды из датасета. Для ускорения расчётов ресэмплируйте дневные данные к недельным с помощью усреднения. В этом разделе будем оценивать только модели без сезонности, так как при усреднении она исчезнет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделите данные на трейн и тест. Последние 16 недель каждого ряда будут тестовой выборкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0.5 балла) Оцените автоматическую ARIMA на трейне каждого ряда. Можете использовать nixtla или pmdarima. Сохраните результат оценки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0.5 балла) Выявите топ-3 самых частых спецификаций рядов. Возьмите из них одну с максимальным количеством параметров. Идея в том, чтобы взять достаточно обобщающую модель для всех рядов в выборке. Будем называть эту модель универсальной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1 балл) Для каждого ряда проведите на тестовой выборке односторонний тест Диболда-Мариано.\n",
    "\n",
    "Нулевая гипотеза: прогнозы универсальной и автоматической моделей не различаются\n",
    "\n",
    "Альтернативная гипотеза: автоматическая модель лучше \n",
    "\n",
    "Выведите долю рядов, для которых гипотеза не отверглась. Получилось ли описать единой моделью значимое количество рядов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Общие тренды на рынке (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом разделе нам необходимо построить прогнозные модели для курсов валют. На семинарах мы обсуждали, что прогнозировать цены котировок стандартными линейными моделями довольно бесперспективно. Так как такие данные близки к модели случайного блуждания, оптимальным прогнозом для них часто оказывается наивный. Однако если перейти к более низкой частоте (например, к месячным данным), то некоторого превосходства над наивной всё же можно добиться.\n",
    "\n",
    "В приложенном датасете currencies.csv находятся следующие величины:\n",
    "1.  Курсы ряда валют по отношению к доллару. \n",
    "\n",
    "    Курс EUR/USD - Евро Доллар США\n",
    "    \n",
    "    Курс CNY/USD - Китайский юань Доллар США\n",
    "    \n",
    "    Курс INR/USD - Индийская рупия Доллар США\n",
    "    \n",
    "    Курс JPY/USD - Японская йена Доллар США\n",
    "    \n",
    "    Курс GBR/USD - Британский фунт Доллар США\n",
    "    \n",
    "    Курс CHF/USD - Швейцарский франк Доллар США\n",
    "    \n",
    "    Курс BRL/USD - Бразильский реал Доллар США\n",
    "    \n",
    "    Курс IDR/USD - Индонезийская рупия Доллар США\n",
    "\n",
    "2. Цена нефти BRENT за баррель и цена газа в Европе за mmbtu (Британская тепловая единица)\n",
    "3. ff_rate -- ставка ФРС США\n",
    "\n",
    "Нашей задачей будет построить прогнозную модель для всех курсов валют из пункта 1. Цены нефти, газа и ставка ФРС даны дополнительно, можете использовать их если посчитаете необходимым, но в базовой версии можно использовать только временные ряды курсов валют."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт данных\n",
    "\n",
    "Импортируйте данные из файла. Приведите информацию о датах в один столбец типа datetime. Например, чтобы каждая точка отображала дату начала месяца. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Писать для каждого ряда одномерную модель несколько расточительно, хотя и не лишено смысла. Этот процесс необходимо автоматизировать. В простом для понимания варианте можно свести прогноз каждой валюты к табличной задаче и подобрать регрессоры из остальных рядов. Мы таким уже занимались, поэтому попробуем зайти с немного другой стороны.\n",
    "\n",
    "Весь рынок и в частности рынок валют -- единый механизм, находящийся в некотором равновесии и иногда от него отклоняющийся. Значит, у рядов может быть общий тренд. Формально такая ситуация называется коинтеграцией и это тема отдельной лекции, нам она будет нужна только для идеи. Более подробно про коинтеграцию можно почитать вот в этом [конспекте](https://vk.com/doc126754362_567660819?hash=AvDGHaO92KX7exjBCleLZsEGHPPX2iZfCqae2Fijkng). \n",
    "\n",
    "### 1. (0.5 балла) Визуализация\n",
    "\n",
    "Давайте увидим это явно. Возьмите все курсы валют, отнормируйте их c помощью StandardScaler из sklearn и изобразите результат на одном графике. \n",
    "\n",
    "График будет немного шумный, но вы должны заметить что в целом валюты движутся по схожим траекториям, а некоторые группируются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно ли выделить эти общие тренды на рынке? Да. Можно просто рассмотреть все валюты как один многомерный вектор и понизить его размерность. Либо оценить VAR-модель и вывести явно коинтеграционное соотношение (уравнение равновесия на рынке). Второй подход нам не подходит из-за малого количества данных для стольких уравнений, поэтому используем первый.\n",
    "\n",
    "Конечно, для временных рядов существуют специфические методы понижения размерности, но они выходят за рамки нашего курса. Поэтому мы воспользуемся не слишком подходящим, но зато простым и знакомым методом главных компонент (PCA). PCA никак не учитывает временную зависимость точек, но мы позволим себе пренебречь этим.\n",
    "\n",
    "### 2. (0.5 балла) Визуализация главных компонент\n",
    "\n",
    "Примените PCA на рядах из восьми валют и отберите три первые главные компоненты. Изобразите их на одном графике. Сравните с предыдущим графиком. Компоненты будут примерно похожи на тренды нескольких разных групп валют."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговое замечание: На рынках есть некоторое глобальное равновесие (коинтеграционное соотношение). Если бы рядов было немного, мы могли бы построить VAR на нестационарных рядах за счёт этого общего тренда. Однако у нас такой возможности нет и придётся колхозить понижение размерности и строить на его основе стационарный VAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Автокодировщик на стероидах (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обойти ограничение на количество рядов. Понизим размерность данных, построим VAR на двух-трёх мерном пространстве, а потом декодируем прогнозы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " Модель будет устроена следующим образом.\n",
    "\n",
    "1. Энкодер. Строим PCA на всех валютах (и доп фичах, если хотите) сразу, выделяем главные компоненты.\n",
    "2. Декодер. PCA не обратим в стандартном понимании, нельзя аналитически получить из главных компонент обратно исходные ряды. Для каждой валюты обучаем регрессионную модель, предсказывающую курс валюты по главным компонентам. Если у нас 8 валют, здесь получится 8 моделей. Модели можно взять любые (линрег, бустинг, ...). Не мучайтесь с подбором гиперпараметров, бустинга из коробки хватит.\n",
    "2. Строим VAR(p)-модель только на главных компонентах\n",
    "4. Прогнозируем главные компоненты вперёд\n",
    "5. Применяем модели-декодеры, чтобы получить из прогнозов главных компоненты прогнозы валют\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0.5 балла) Необходимо сделать препроцессинг данных, так как нам нужно будет строить VAR на стационарных рядах. Мы не уверены, что на PCA-рядах будет существовать коинтеграция. Приведите все ряды к стационарному виду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (0.5 балла) Реализуйте метод fit_pca, который вычисляет главные компоненты от всех валют и дополнительных переменных и сохраняет их. Компонент может быть не более трёх, либо не более 95% объяснённой дисперсии.\n",
    "\n",
    "2. (0.5 балла) Реализуйте метод fit_var. В нём необходимо обучить VAR(p)-модель на главных компонентах из предыдущего пункта.\n",
    "\n",
    "3. (1 балл) Реализуйте метод fit_decoders. Необходимо оценить ряд регрессий, восстанавливающих валюты из главных компонент. Восстанавливать дополнительные переменные не нужно. Заморачиваться с отбором параметров этих моделей не нужно (только при большом желании). Бустинга из коробки должно хватить.\n",
    "\n",
    "4. (1 балл) Реализуйте метод forecast. VAR прогнозируют главные компоненты, а модели-декодеры восстанавливают из этих прогнозов валюты. Функция должна возвращать датафрейм прогнозов всех валют на 12 шагов. По строкам идут валюты, по столбцам -- горизонты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "import pandas as pd\n",
    "\n",
    "class EncoderDecoderForecaster:\n",
    "    \n",
    "    def __init__(self, data:pd.DataFrame, p: int|None = None):\n",
    "        \"\"\"\n",
    "        __init__ Initializing forecaster class\n",
    "\n",
    "        Arguments:\n",
    "            data {pd.DataFrame} -- Dataframe of all data\n",
    "            p {int} -- Order of VAR model\n",
    "        \"\"\"\n",
    "        self.is_pca_fitted = False\n",
    "        self.is_var_fitted = False\n",
    "\n",
    "        self.data = data \n",
    "        self.p = p\n",
    "\n",
    "        self.decoders = []\n",
    "        \n",
    "        \n",
    "    def fit_pca(self, covered_variance=0.95, max_components=3) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        fit_pca Fitting PCA on a bunch of currencies\n",
    "\n",
    "        Arguments:\n",
    "            features {pd.DataFrame} -- Dataframe of features\n",
    "\n",
    "        Keyword Arguments:\n",
    "            covered_variance {float} -- Share of variance, covered by PCA from original data. (default: {0.95})\n",
    "            max_components {int} -- Maximum number of PCA components, allowed for VAR. Have more priority than covered_variance.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame -- Dataframe of principal components\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Your code\n",
    "        \n",
    "        self.is_pca_fitted = True\n",
    "\n",
    "\n",
    "    def fit_var(self):\n",
    "\n",
    "        \"\"\"\n",
    "         Fitting var on results of PCA\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.is_pca_fitted\n",
    "\n",
    "        self.model = ... # Your code\n",
    "        \n",
    "\n",
    "        self.is_var_fitted = True\n",
    "\n",
    "    def fit_decoders(self):\n",
    "\n",
    "        \"\"\"\n",
    "         Fitting decoders from PCA to original data\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Your code\n",
    "\n",
    "        assert len(self.decoders) > 0\n",
    "        self.is_decoder_fitted = True\n",
    "\n",
    "\n",
    "    def forecast(self, h: int = 12) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        forecast Forecast VAR and decode it's forecasts with decoder-models.\n",
    "\n",
    "        Keyword Arguments:\n",
    "            h {int} -- Forecasting horizon (default: {1})\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame -- Forecasts of all currencies. Horizon by columns. Currencies by rows.\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.is_pca_fitted and self.is_var_fitted and self.is_decoder_fitted\n",
    "\n",
    "        # Your code\n",
    "\n",
    "        forecast = ...\n",
    "\n",
    "        assert isinstance(forecast, pd.DataFrame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. (1 балл) Отберите параметр p и посчитайте прогнозную ошибку такого метода для каждой валюты. \n",
    "\n",
    "    Предлагается следующая процедура. Пусть n - количество переменных в нашей модели (валюты + экзогенные по необходимости).\n",
    "\n",
    "    1. Выбираем некоторое стартовое окно. У нас 283 точки. 36 (12*3) последних точек выделим на тестовую часть, остальное на тренировочную.\n",
    "    2. На тренировочной части выбираем p по информационному критерию. В классе выше это означает p = None\n",
    "\n",
    "    Далее попробуем получить наиболее репрезентативную оценку ошибки для такого p. Мы будем сразу считать относительную ошибку в сравнении с наивной моделью, так как на котировках часто нельзя построить статистическую модель лучше наивной. \n",
    "\n",
    "    1. Прогнозируем полученной моделью на 12 шагов вперёд по каждой валюте. (матрица размера [n, 12])\n",
    "    2. Считаем абсолютную ошибку прогноза по каждой валюте и горизонту (матрица размера [n, 12])\n",
    "    3. Строим наивный прогноз по каждой валюте  (матрица размера [n, 12]).\n",
    "    4. Считаем абсолютную ошибку наивного прогноза по каждой валюте и горизонту  (матрица размера [n, 12])\n",
    "    5. Считаем отношение ошибки нашего прогноза к наивной ошибке (поэлементно, матрица размера [n, 12])\n",
    "    7. Увеличиваем тренировочную выборку на 6 наблюдений. Переоцениваем всю модель на новых данных, но уже при фиксированном p. В классе выше это будет p={некоторое число}\n",
    "\n",
    "    Повторяем эту процедуру пока не закончатся данные. При текущих параметрах получится 4 итерации. Усредняем матрицы относительных ошибок поэлементно по всем итерациям. Итого получаем матрицу размера [n, 12].\n",
    "    \n",
    "    \n",
    "    Параметры окон и горизонт можете поменять по собственным соображениям, но поясните логику.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_cross_val_score_encoder_decoder(data: pd.DataFrame, initial_window_size: int, step: int, p: int, horizon: int = 12) -> pd.Series:\n",
    "    \"\"\"\n",
    "    cross_val_score Estimating MAE cross-val score on a fitted model. Needed to get more sustainable MAE estimation\n",
    "\n",
    "    Arguments:\n",
    "        initial_window_size {int} -- Initial size of expanding window\n",
    "        step {int} -- Step size of expanding window\n",
    "        horizon {int} -- Forecasting horizon of cross-validation score\n",
    "        p {int} -- order of VAR model\n",
    "    Returns:\n",
    "        pd.DataFrame -- Frame of absolute error by each horizon and each currency, averaged by several folds\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Your code\n",
    "\n",
    "    score = ...\n",
    "\n",
    "    assert isinstance(score, pd.DataFrame)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. (0.5 балла) Постройте графики скоров.\n",
    "\n",
    "    1.  Каждая валюта представлена линией на графике\n",
    "    2.  По оси абсцисс идёт горизонт прогнозирования (от 1 до 12)\n",
    "    3.  По оси ординат отношение ошибки VAR-модели к ошибке наивной модели\n",
    "\n",
    "Получилось ли по какой-то валюте стабильно предсказывать лучше наивной на всех горизонтах. По каким валютам получилось лучше, по каким хуже?\n",
    "\n",
    "У нас получалось, что на сильно зарегулированных экономиках (например, Китай) прогнозы близки к наивным, так как курс валюты сильнее зависит от решений партии, а не внешней обстановки. Ваша постановка и данные несколько упрощены, поэтому не факт, что у вас получится воспроизвести этот эффект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Расширения модели (До 3 баллов в зависимости от сложности модификации)\n",
    "\n",
    "PCA -- не очень честный метод для временных рядов. Предложите, как можно скорректировать процедуру либо найдите метод понижения размерности специально для временных рядов. Кратко опишите суть и примените его для обоих классов выше вместо PCA. Получилось ли улучшить качество модели?\n",
    "\n",
    "1. (1 балла) Ваше краткое описание методики и почему она подходит для рядов. Если есть статьи, можете приложить ссылки.\n",
    "\n",
    "2. (2 балла) Реализация, подсчёт метрик и анализ результата."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Рубрика \"как вам домашка?\" (0.1 балла)\n",
    "\n",
    "Пройдите короткий опрос. Это действительно важно. https://forms.gle/w3sV453spERTbGvr7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
