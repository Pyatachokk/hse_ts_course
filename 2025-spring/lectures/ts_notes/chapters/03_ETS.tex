\documentclass[12pt,fleqn]{article}
\usepackage{../../vkCourseML}
%\usepackage{vkCourseML}
\hypersetup{unicode=true}
%\usepackage[a4paper]{geometry}
\usepackage[hyphenbreaks]{breakurl}

\interfootnotelinepenalty=10000
\newcommand{\dx}[1]{\,\mathrm{d}#1} % маленький отступ и прямая d

\begin{document}
\section{Модели экспоненциального сглаживания}

\subsection{Простое экспоненциальное сглаживание}

Предположим, что мы хотим спрогнозировать некоторый временной ряд $(y_t)_{t=1}^{T}$. Также предположим, что данный ряд не имеет выраженной сезонности или тренда. Самой простой моделью прогнозирования можно считать наивную:

\begin{equation}
\hat{y}_{T+1|T} = y_T
\end{equation}

Данная модель хорошо подходит для бенчмарка, но в большинстве случаев (не всегда!) слишком проста для прогнозирования. Как минимум, она никак не учитывает историю до $y_T$. Попробуем это исправить. Например, можно добавить усреднение всей истории.


\begin{equation}
\hat{y}_{T+1|T} = \frac{1}{T}\sum_{t=1}^{T}y_t
\end{equation}

Мы добавили зависимость от истории, однако перестарались. Все наблюдения в таком случае будут иметь одинаковый вес. Логично предположить, что наблюдения, близкие к моменту времени $T$ должны иметь больший вес. Например, если в далёком прошлом, близко к моменту времени $1$ временной ряд имел выбросы или структурные сдвиги, не хотелось бы придавать этому большой вес. Сама собой напрашивается геометрическая прогрессия с убывающими весами. Зададим параметр $\alpha \in [0, 1]$ как вес наблюдения $y_T$ и будем уменьшать его на вес $q$.

\begin{equation}
\hat{y}_{T+1|T} =\sum_{i=0}^{T-1} \alpha q^{i} y_{T-i}
\end{equation}

Найдём веса q. Для простоты предположим, что их сумма должна равняться 1. 

\begin{equation}
\alpha + q \alpha + q^2 \alpha + \ldots + q^{T-1} \alpha = 1
\end{equation}

Однако полученное для суммы этой прогрессии уравнение будет  зависеть от $T$ и решать его не очень удобно:

\begin{equation}
\frac{\alpha(q^T - 1)}{q-1} = 1
\end{equation}

Для упрощения предположим, что $T$ велико и воспользуемся бесконечно убывающей геометрической прогрессией:

\begin{equation}
\frac{\alpha}{q-1} = 1 \Rightarrow q = 1-\alpha
\end{equation}

Конечно, у нас в реальном мире не бесконечное количество данных и это будет приближением, но довольно точным. Таким образом мы получим финальную форму модели:

\begin{equation}
\hat{y}_{T+1|T} = \sum_{i=0}^{T-1} \alpha (1-\alpha)^{i} y_{T-i}
\end{equation}

Веса $ \alpha (1-\alpha)^{t-1} $ убывают экспоненциально с ростом $t$, откуда и получила название модель простого экспоненциального сглаживания. Многошаговый прогноз такой модели будет плоским и будет просто повторять одношаговый прогноз:

\begin{equation}
\hat{y}_{T+h|T} = \sum_{i=0}^{T-1} \alpha (1-\alpha)^{i} y_{T-i}
\end{equation}

Параметр $\alpha$ можно подобрать, численно решив следующую задачу оптимизации:

$$
\sum_{t=1}^{T}(y_t - \hat{y}_{t|t-1})^2 \rightarrow \min_\alpha
$$

Для дальнейшего анализа будет полезно рассмотрерь несколько дополнительных форм модели экспоненциального сглаживания.

\subsubsection{Модель коррекции ошибок}

Сгруппируем последнее выражение относительно $\alpha$:

\begin{equation}
	\label{error_correction}
	\begin{split}
		\hat{y}_{T+1|T} 
		&= \alpha y_t + \alpha (1-\alpha) y_{t-1} + \alpha (1-\alpha)^2 y_{t-2} + \ldots \\
		&= \alpha y_t +(1-\alpha)[ \alpha y_{t-1} + \alpha (1-\alpha) y_{t-2} + \ldots ] \\
		&= \alpha y_T +(1-\alpha) \hat{y}_{T|T-1} \\
		&= \alpha (y_T - \hat{y}_{T|T-1}) + \hat{y}_{T|T-1} \\
		&= \alpha e_T + \hat{y}_{T|T-1}
	\end{split}
\end{equation}

Из этой записи следует, что прогноз можно представить как коррекцию предыдущего прогноза на его ошибку относительно истинного значения с некоторым коэффициентом. Сейчас нам этот результат интересен скорее как занимательный факт, но в дальнейшем похожая идея будет использоваться в модели VECM (Vector Error Correction Model).


\subsubsection{Взвешенное среднее}

Воспользуемся результатом из уравнения \ref{error_correction}.

\begin{equation}
\begin{split}
	\hat{y}_{t+1|t} 
	&= \alpha y_t + \alpha (1-\alpha) y_{t-1} + \alpha (1-\alpha)^2 y_{t-2} + \ldots \\
	&= \alpha y_t +(1-\alpha)[ \alpha y_{t-1} + \alpha (1-\alpha) y_{t-2} + \ldots ] \\
	&=  \alpha y_t +(1-\alpha) \hat{y}_{t|t-1}
\end{split}
\end{equation}

Получается, что наш прогноз можно представить как взвешенное среднее наблюдаемого значения $y_T$ и его прогноза, полученного на предыдущем шаге $\hat{y}_{T|T-1}$. Однако надо заметить, что для корректности такой формы нужно ввести один дополнительный параметр $l_0$, инициализирующий последовательность. Далее станет ясно, почему в качестве имени мы взяли именно $l_0$.

$$
 \hat{y}_{2|1} = \alpha y_1 +(1-\alpha) l_0
$$

Тогда прогнозное уравнение также изменится.

$$
\hat{y}_{T+1|T} = \frac{1}{T}\sum_{i=0}^{T-1} \alpha (1-\alpha)^{i} y_{T-i} + (1-\alpha)^T l_0
$$

Вес последнего слагаемого будет быстро убывать при больших T, и модель будет эквивалентна стандартной постановке. Для полной эквивалентности можно просто положить $l_0 = 0$.

Параметр $l_0$ можно найти из той же задачи оптимизации:

$$
\sum_{t=1}^{T}(y_t - \hat{y}_{t|t-1})^2 \rightarrow \min_{\alpha, l_0}
$$


\subsubsection{Компонентный вид}

Вы часто думаете о Римской империи? Для дальнейших выводов воспользуемся древним римским принципом "разделяй и властвуй". Разобьём наше уравнение на два:

\begin{equation}
\begin{array}{llll}
	&\text{Уравнение прогноза} \quad & 	\hat{y}_{t+1|t}& = l_t \\
	&\text{Уравнение сглаживания}\quad & l_t& =\alpha y_t +(1-\alpha) l_{t-1}
\end{array}
\end{equation}

Эта формулировка эквивалентна предыдущим. Она удобна технически для того, чтобы впоследствии добавлять уравнения и новые компоненты в уравнение прогноза. Здесь мы также заострим внимание на том, что $l_t$ в такой постановке можно интерпретировать как сглаженный уровень ряда.



\subsection{Трендированные модели}

Предыдущая модель подходит только для данных без ярко выраженных трендов.
Для добавления большей динамики введём ещё один показатель.
$b_t$ будет означать локальную скорость роста за один период модели.
Грубо говоря, этот параметр будет отвечать за приращения компоненты $l_t$. Обновлённая система уравнений будет выглядеть следующим образом.

\begin{equation}
\begin{array}{llll}
		&\text{Уравнение прогноза} \quad & 	\hat{y}_{t+h|t}& = l_t + h b_t \\
		&\text{Уравнение сглаживания}\quad & l_t& =\alpha y_t +(1-\alpha)(l_{t-1} + b_{t-1}) \\
		&\text{Уравнение тренда}\quad & b_t& =\beta (l_t - l_{t-1}) +(1-\beta) b_{t-1}
\end{array}
\end{equation}

В последнем уравнении мы усредняем оценку тренда на основе приращений $(l_t - l_{t-1})$ и предыдущую оценку $b_{t-1}$. Добавление уравнения также увеличивает количество параметров, к уже имеющемуся списку прибавим $\beta$ и $b_0$.

Важно отметить, что несмотря на долгую историю своего существования, этот метод не является сакральным эталоном и он довольно эвристичен. 
Никто не мешает модифицировать эти формулы в зависимости от вашего процесса или внутренних убеждений. 
Например, приращения можно оценивать на основе истинных данных $(y_t - y_{t-1})$, а не сглаженных. 
Всё на ваше творческое усмотрение, мы лишь описываем модели, некогда оказавшиеся удачными.

Можно заметить, что прогноз из плоского стал линейным. Подобная линейная экстраполяция может быть плоха по ряду причин. Во-первых, тренды могут менять направление на прогнозном горизонте. С этим ничего не поделать силами такой простой модели, но от неё это и не требуется. Во-вторых, эмпирически установлено, что модели линейного тренда склонны переоценивать тренд на больших горизонтах. Проще говоря, на практике тренды, близкие к линейным, склонны затухать. Если ваш ряд растёт экспоненциально, то скорее всего затухать будет его логарифм. Предлагается штрафовать модель на небольшой коэффициент $\phi \in [0, 1]$ за каждый последующий шаг.


\begin{equation}
	\begin{split}
		\hat{y}_{t+h|t } &= l_t + (\phi + \phi^2 + \ldots + \phi^h) b_t \\
		l_t &=\alpha y_t +(1-\alpha)(l_{t-1} + \phi b_{t-1}) \\
		b_t &=\beta (l_t - l_{t-1}) +(1-\beta) \phi b_{t-1}
	\end{split}
\end{equation}

Для далёких горизонтов значение прогноза будет выходить на константу. Однако в целом не рекомендуется слишком сильно полагаться на модель на больших горизонтах, так как дисперсия прогнозов растёт довольно быстро. Это нельзя увидеть явно без введения вероятностной модели, но вскоре мы до этого доберёмся.

\[ \lim\limits_{h\rightarrow\infty} 	\hat{y}_{t+h|t} =  l_t + \frac{\phi b_t}{1-\phi} \quad \text{при} \quad \phi \in (0, 1)\]

Параметр $\phi$ также можно оценить с помощью задачи оптимизации.


\subsection{Сезонные модели}

Добавим в нашу римско-имперскую модель ещё одно уравнение на сезонность: $s_t$.  Для этого нужно выбрать период сезонности $m$. В базовом варианте модель предполагает одну сезонность. 

Для начала создадим само уравнение сезонности. Сезонность предполагается цикличной и мало изменяющейся компонентой. Как и все предыдущие уравнения, будем строить его как взвешенное среднее между модельной и наблюдаемой величинами. 

Наблюдаемую сезонность можно выделить как разность $(y_t - l_{t-1} - b_{t-1})$. Так как сезонность предполагается цикличной, в качестве модельной величины возьмём просто предыдущее значение $s_{t-m}$.

Также модифицируем уравнение сглаживания. По нашим предпосылка $l_t$ является некоторым сглаженным уровнем ряда $y_t$. Значит в фактической части уравнения нужно очистить $y_t$ от сезонности.

Наконец, модифицируем уравнение прогноза. Для прогноза будем использовать последний сезонный цикл тренировочных данных. Так, если мы прогнозируем месячные данные на декабрь, в качестве сезонной компоненты прогноза используем последний декабрь из выборки. Формула в данном случае оказывается сложнее идеи: $s_{t+h-m(k+1)}$, где $k$ = 

\begin{equation}
	\begin{array}{llll}
		&\text{Уравнение прогноза} \quad & 	\hat{y}_{t+h|t}& = l_t + h b_t + s_{t+h-m(k+1)}\\
		&\text{Уравнение сглаживания}\quad & l_t& =\alpha (y_t - s_{t-m}) +(1-\alpha)(l_{t-1} + b_{t-1}) \\
		&\text{Уравнение тренда}\quad & b_t& =\beta (l_t - l_{t-1}) +(1-\beta) b_{t-1} \\
		&\text{Уравнение сезонности}\quad & s_t& =\gamma (y_t - l_{t-1} - b_{t-1}) +(1-\gamma) s_{t-m}
	\end{array}
\end{equation}

Также для уравнения сезонности часто используется аналогичная формулировка:

\begin{equation}
s_t  =\gamma^{\ast} (y_t - l_{t}) +(1-\gamma^{\ast}) s_{t-m}
\end{equation}

Подставив в это уравнение $l_t$, легко убедиться, что $\gamma = \gamma^{\ast}(1 - \alpha)$.

Для сезонной модели необходимо сделать два важных замечания.

\begin{enumerate}
	\item Для уравнения сезонности также необходимо ввести стартовые параметры. Заметьте, что для корректной оценки необходимо ввести $m$ параметров: $s_1, s_2, \ldots, s_m$
	
	\item Так как сезонность -- цикличная компонента, и притом в нашей постановке аддитивная, введём следующее ограничение:
	
	\[
	s_1 + \ldots + s_m = 0
	\]
	
	\item Ограничение из предыдущего пункта позволяет выразить один параметр через все остальные. Следовательно, для задачи достаточно определить не $m$, а $m-1$ параметров.
\end{enumerate}
\subsection{Мультипликативные модели}

\subsection{ETS}

Все рассмотренные до этого модели не были стохастическими. Для них можно было просто выписать функционал на основе MSE и оптимизировать градиентным спуском. Однако на все описанные модели путём нескольких простых переходов можно навесить стохастику. Приведём пример для самой простой версии экспоненциального сглаживания:

\begin{equation}
	\begin{array}{llll}
		&\text{Уравнение прогноза} \quad & 	\hat{y}_{t+1|t}& = l_t \\
		&\text{Уравнение сглаживания}\quad & l_t& =\alpha y_t +(1-\alpha) l_{t-1}
	\end{array}
\end{equation}

Вспомним, что уравнение сглаживания можно переписать в форме error correction:

\begin{equation}
   l_t = \alpha y_t +(1-\alpha) l_{t-1} =  l_{t-1}  + \alpha (y_t - l_{t-1}) =  l_{t-1}  + \alpha e_t,
\end{equation}

где $e_t = y_t - l_{t-1} = y_t - \hat{y}_{t|t-1}$

Иходя из определения $e_t$:

\begin{equation}
y_t = l_{t-1} + e_t
\end{equation}

Достаточно естественно в данной постановке моделировать $e_t$ через распределение. Например, $e_t = \varepsilon_t = \mathcal{N}(0, \sigma^2)$.

Таким образом, итоговая модель:

\begin{equation}
	\begin{array}{llll}
		&\text{Уравнение прогноза} \quad & 	\hat{y}_{t+1|t}& = l_{t-1} + \varepsilon_t \\
		&\text{Уравнение сглаживания}\quad & l_t & = l_{t-1}  + \alpha \varepsilon_t,
	\end{array}
\end{equation}




\end{document}