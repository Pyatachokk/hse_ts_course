% !TEX root = ../ts_notes.tex
\documentclass[12pt,fleqn]{article}
\usepackage{../vkCourseML}
\newcounter{chapter}
\hypersetup{unicode=true}
%\usepackage[a4paper]{geometry}
% \usepackage[hyphenbreaks]{breakurl}  % Not needed with pdflatex
\interfootnotelinepenalty=10000
\newcommand{\dx}[1]{\,\mathrm{d}#1} % маленький отступ и прямая d

\begin{document}
\setcounter{figure}{0}
\counterwithin{figure}{chapter}
\section{Модели экспоненциального сглаживания}
\subsection{Простое экспоненциальное сглаживание}

Предположим, что мы хотим спрогнозировать некоторый временной ряд $(y_t)_{t=1}^{T}$. Также предположим, что данный ряд не имеет выраженной сезонности или тренда. Самой простой моделью прогнозирования можно считать наивную:

\begin{equation}
  \hat{y}_{T+1|T} = y_T
\end{equation}

Данная модель хорошо подходит для бенчмарка, но в большинстве случаев (не всегда!) слишком проста для прогнозирования. Как минимум, она никак не учитывает историю до $y_T$. Попробуем это исправить. Например, можно добавить усреднение всей истории.

\begin{equation}
  \hat{y}_{T+1|T} = \frac{1}{T}\sum_{t=1}^{T}y_t
\end{equation}

Мы добавили зависимость от истории, однако перестарались. Все наблюдения в таком случае будут иметь одинаковый вес. Логично предположить, что наблюдения, близкие к моменту времени $T$ должны иметь больший вес. Например, если в далёком прошлом, близко к моменту времени $1$ временной ряд имел выбросы или структурные сдвиги, не хотелось бы придавать этому большой вес. Сама собой напрашивается геометрическая прогрессия с убывающими весами. Зададим параметр $\alpha \in [0, 1]$ как вес наблюдения $y_T$ и будем уменьшать его на вес $q$.

\begin{equation}
  \hat{y}_{T+1|T} =\sum_{i=0}^{T-1} \alpha q^{i} y_{T-i}
\end{equation}

Найдём веса q. Для простоты предположим, что их сумма должна равняться 1.

\begin{equation}
  \alpha + q \alpha + q^2 \alpha + \ldots + q^{T-1} \alpha = 1
\end{equation}

Однако полученное для суммы этой прогрессии уравнение будет  зависеть от $T$ и решать его не очень удобно:

\begin{equation}
  \frac{\alpha(1-q^T)}{1-q} = 1
\end{equation}

Для упрощения предположим, что $T$ велико и воспользуемся бесконечно убывающей геометрической прогрессией:

\begin{equation}
  \frac{\alpha}{1-q} = 1 \Rightarrow q = 1-\alpha
\end{equation}

Конечно, у нас в реальном мире не бесконечное количество данных и это будет приближением, но довольно точным. Таким образом мы получим финальную форму модели:

\begin{equation}
  \hat{y}_{T+1|T} = \sum_{i=0}^{T-1} \alpha (1-\alpha)^{i} y_{T-i}
\end{equation}

Веса $ \alpha (1-\alpha)^{t-1} $ убывают экспоненциально с ростом $t$, откуда и получила название модель простого экспоненциального сглаживания. Многошаговый прогноз такой модели будет плоским и будет просто повторять одношаговый прогноз:

\begin{equation}
  \hat{y}_{T+h|T} = \sum_{i=0}^{T-1} \alpha (1-\alpha)^{i} y_{T-i}
\end{equation}

Параметр $\alpha$ можно подобрать, численно решив следующую задачу оптимизации:

\begin{equation}
  \sum_{t=1}^{T}(y_t - \hat{y}_{t|t-1})^2 \rightarrow \min_\alpha
\end{equation}

Для дальнейшего анализа будет полезно рассмотреть несколько дополнительных форм модели экспоненциального сглаживания.

\subsubsection{Модель коррекции ошибок}

Сгруппируем последнее выражение относительно $\alpha$:

\begin{equation}
  \label{error_correction}
  \begin{split}
    \hat{y}_{T+1|T}
    &= \alpha y_T + \alpha (1-\alpha) y_{T-1} + \alpha (1-\alpha)^2 y_{T-2} + \ldots \\
    &= \alpha y_T +(1-\alpha)[ \alpha y_{T-1} + \alpha (1-\alpha) y_{T-2} + \ldots ] \\
    &= \alpha y_T +(1-\alpha) \hat{y}_{T|T-1} \\
    &= \alpha (y_T - \hat{y}_{T|T-1}) + \hat{y}_{T|T-1} \\
    &= \alpha e_T + \hat{y}_{T|T-1}
  \end{split}
\end{equation}

Из этой записи следует, что прогноз можно представить как коррекцию предыдущего прогноза на его ошибку относительно истинного значения с некоторым коэффициентом. Сейчас нам этот результат интересен скорее как занимательный факт, но в дальнейшем похожая идея будет использоваться в модели VECM (Vector Error Correction Model).

\subsubsection{Взвешенное среднее}

Воспользуемся результатом из уравнения \ref{error_correction}.

\begin{equation}
  \begin{split}
    \hat{y}_{t+1|t}
    &= \alpha y_t + \alpha (1-\alpha) y_{t-1} + \alpha (1-\alpha)^2 y_{t-2} + \ldots \\
    &= \alpha y_t +(1-\alpha)[ \alpha y_{t-1} + \alpha (1-\alpha) y_{t-2} + \ldots ] \\
    &=  \alpha y_t +(1-\alpha) \hat{y}_{t|t-1}
  \end{split}
\end{equation}

Получается, что наш прогноз можно представить как взвешенное среднее наблюдаемого значения $y_T$ и его прогноза, полученного на предыдущем шаге $\hat{y}_{T|T-1}$. Однако надо заметить, что для корректности такой формы нужно ввести один дополнительный параметр $l_0$, инициализирующий последовательность. Далее станет ясно, почему в качестве имени мы взяли именно $l_0$.

\begin{equation}
  \hat{y}_{2|1} = \alpha y_1 +(1-\alpha) l_0
\end{equation}

Тогда прогнозное уравнение также изменится.

\begin{equation}
  \hat{y}_{T+1|T} = \sum_{i=0}^{T-1} \alpha (1-\alpha)^{i} y_{T-i} + (1-\alpha)^T l_0
\end{equation}

Вес последнего слагаемого будет быстро убывать при больших T, и модель будет эквивалентна стандартной постановке. Для полной эквивалентности можно просто положить $l_0 = 0$.

Параметр $l_0$ можно найти из той же задачи оптимизации:

\begin{equation}
  \sum_{t=1}^{T}(y_t - \hat{y}_{t|t-1})^2 \rightarrow \min_{\alpha, l_0}
\end{equation}

\subsubsection{Компонентный вид}

Для дальнейших выводов воспользуемся принципом "разделяй и властвуй". Разобьём наше уравнение на два:

\begin{equation}
  \begin{array}{llll}
    &\text{Уравнение прогноза} \quad &   \hat{y}_{t+1|t} & = l_t \\
    &\text{Уравнение сглаживания}\quad & l_t & =\alpha y_t +(1-\alpha) l_{t-1}
  \end{array}
\end{equation}

Эта формулировка эквивалентна предыдущим. Она удобна технически для того, чтобы впоследствии добавлять уравнения и новые компоненты в уравнение прогноза. Здесь мы также заострим внимание на том, что $l_t$ в такой постановке можно интерпретировать как сглаженный уровень ряда.

\subsection{Трендированные модели}

Предыдущая модель подходит только для данных без ярко выраженных трендов.
Для добавления большей динамики введём ещё один показатель.
$b_t$ будет означать локальную скорость роста за один период модели.
Грубо говоря, этот параметр будет отвечать за приращения компоненты $l_t$. Обновлённая система уравнений будет выглядеть следующим образом.

\begin{equation}
  \begin{array}{llll}
    &\text{Уравнение прогноза} \quad &   \hat{y}_{t+h|t} & = l_t + h b_t \\
    &\text{Уравнение сглаживания}\quad & l_t & =\alpha y_t +(1-\alpha)(l_{t-1} + b_{t-1}) \\
    &\text{Уравнение тренда}\quad & b_t & =\beta (l_t - l_{t-1}) +(1-\beta) b_{t-1}
  \end{array}
\end{equation}

В последнем уравнении мы усредняем оценку тренда на основе приращений $(l_t - l_{t-1})$ и предыдущую оценку $b_{t-1}$. Добавление уравнения также увеличивает количество параметров, к уже имеющемуся списку прибавим $\beta$ и $b_0$.

Важно отметить, что несмотря на долгую историю своего существования, этот метод не является сакральным эталоном и он довольно эвристичен.
Никто не мешает модифицировать эти формулы в зависимости от вашего процесса или внутренних убеждений.
Например, приращения можно оценивать на основе истинных данных $(y_t - y_{t-1})$, а не сглаженных.
Всё на ваше творческое усмотрение, мы лишь описываем модели, некогда оказавшиеся удачными.

Можно заметить, что прогноз из плоского стал линейным. Подобная линейная экстраполяция может быть плоха по ряду причин. Во-первых, тренды могут менять направление на прогнозном горизонте. С этим ничего не поделать силами такой простой модели, но от неё это и не требуется. Во-вторых, эмпирически установлено, что модели линейного тренда склонны переоценивать тренд на больших горизонтах. Проще говоря, на практике тренды, близкие к линейным, склонны затухать. Если ваш ряд растёт экспоненциально, то скорее всего затухать будет его логарифм. Предлагается штрафовать модель на небольшой коэффициент $\phi \in [0, 1]$ за каждый последующий шаг.

\begin{equation}
  \begin{split}
    \hat{y}_{t+h|t } &= l_t + (\phi + \phi^2 + \ldots + \phi^h) b_t \\
    l_t &=\alpha y_t +(1-\alpha)(l_{t-1} + \phi b_{t-1}) \\
    b_t &=\beta (l_t - l_{t-1}) +(1-\beta) \phi b_{t-1}
  \end{split}
\end{equation}

Для далёких горизонтов значение прогноза будет выходить на константу. Однако в целом не рекомендуется слишком сильно полагаться на модель на больших горизонтах, так как дисперсия прогнозов растёт довольно быстро. Это нельзя увидеть явно без введения вероятностной модели, но вскоре мы до этого доберёмся.

\begin{equation}
  \lim\limits_{h\rightarrow\infty}   \hat{y}_{t+h|t} =  l_t + \frac{\phi b_t}{1-\phi} \quad \text{при} \quad \phi \in (0, 1)
\end{equation}

Параметр $\phi$ также можно оценить с помощью задачи оптимизации.

\subsection{Сезонные модели}

Добавим в нашу римско-имперскую модель ещё одно уравнение на сезонность: $s_t$.  Для этого нужно выбрать период сезонности $m$. В нашей постановке модель предполагает одну сезонность.

\subsection{Аддитивная сезонность}

Для начала создадим само уравнение сезонности. Сезонность предполагается цикличной и мало изменяющейся компонентой. Как и все предыдущие уравнения, будем строить его как взвешенное среднее между модельной и наблюдаемой величинами.

Наблюдаемую сезонность можно выделить как разность $(y_t - l_{t-1} - b_{t-1})$. Так как сезонность предполагается цикличной, в качестве модельной величины возьмём просто предыдущее значение $s_{t-m}$.

Также модифицируем уравнение сглаживания. По нашим предпосылка $l_t$ является некоторым сглаженным уровнем ряда $y_t$. Значит в фактической части уравнения нужно очистить $y_t$ от сезонности.

Наконец, модифицируем уравнение прогноза. Для прогноза будем использовать последний сезонный цикл тренировочных данных. Так, если мы прогнозируем месячные данные на декабрь, в качестве сезонной компоненты прогноза используем последний декабрь из выборки. Формула в данном случае оказывается сложнее идеи: $s_{t+h-m(k+1)}$, где $k$ — целая часть от $(h-1)/m$.

Параметр $k$ определяет, какой сезонный цикл использовать для прогнозирования. При прогнозировании на $h$ шагов вперёд, значение $k = \lfloor(h-1)/m\rfloor$ указывает, на сколько полных сезонных циклов назад нужно отступить, чтобы использовать последний тренировочный цикл. Например, при месячных данных ($m=12$):

\begin{itemize}
  \item при прогнозировании на $h=1$ шаг вперёд: $k = \lfloor(1-1)/12\rfloor = 0$, используем сезонную компоненту из последнего цикла;
  \item при прогнозировании на $h=15$ шагов вперёд: $k = \lfloor(15-1)/12\rfloor = 1$, используем сезонную компоненту из предпоследнего цикла;
  \item при прогнозировании на $h=25$ шагов вперёд: $k = \lfloor(25-1)/12\rfloor = 2$, используем сезонную компоненту из третьего с конца цикла.
\end{itemize}

\begin{equation}
  \begin{array}{llll}
    &\text{Уравнение прогноза} \quad &   \hat{y}_{t+h|t} & = l_t + h b_t + s_{t+h-m(k+1)}\\
    &\text{Уравнение сглаживания}\quad & l_t & =\alpha (y_t - s_{t-m}) +(1-\alpha)(l_{t-1} + b_{t-1}) \\
    &\text{Уравнение тренда}\quad & b_t & =\beta (l_t - l_{t-1}) +(1-\beta) b_{t-1} \\
    &\text{Уравнение сезонности}\quad & s_t & =\gamma (y_t - l_{t-1} - b_{t-1}) +(1-\gamma) s_{t-m}
  \end{array}
\end{equation}

Также для уравнения сезонности часто используется аналогичная формулировка:

\begin{equation}
  s_t  =\gamma^{\ast} (y_t - l_{t}) +(1-\gamma^{\ast}) s_{t-m}
\end{equation}

Подставив в это уравнение $l_t$, легко убедиться, что:

\begin{equation}
  s_t = \gamma^* (y_t - l_{t-1} - b_{t-1}) + [1 - \gamma^*] s_{t-m}
\end{equation}

при $\gamma = \gamma^*(1-\alpha)$. Стандартное ограничение на параметры $0 \leq \gamma^* \leq 1$ трансформируется в $0 \leq \gamma \leq 1-\alpha$.

Для сезонной модели необходимо сделать несколько важных замечаний.

\begin{enumerate}
  \item Для уравнения сезонности также необходимо ввести стартовые параметры. Заметьте, что для корректной оценки необходимо ввести $m$ параметров: $s_1, s_2, \ldots, s_m$

  \item Так как сезонность -- цикличная компонента, и притом в нашей постановке аддитивная, введём следующее ограничение:

    \begin{equation}
      s_1 + \ldots + s_m = 0
    \end{equation}

  \item Ограничение из предыдущего пункта позволяет выразить один параметр через все остальные. Следовательно, для задачи достаточно определить не $m$, а $m-1$ параметров.
\end{enumerate}

\subsubsection{Мультипликативная сезонность}

В отличие от аддитивной сезонности, где сезонная компонента прибавляется к уровню и тренду, в мультипликативной модели сезонность умножается на уровень и тренд. Это означает, что амплитуда сезонных колебаний пропорциональна уровню ряда. Такой подход более уместен, когда сезонные колебания увеличиваются или уменьшаются в зависимости от уровня ряда.

Модель с мультипликативной сезонностью имеет следующий вид:

\begin{equation}
  \begin{array}{llll}
    &\text{Уравнение прогноза} \quad &   \hat{y}_{t+h|t} & = (l_t + h b_t) \cdot s_{t+h-m(k+1)}\\
    &\text{Уравнение сглаживания}\quad & l_t & =\alpha \frac{y_t}{s_{t-m}} +(1-\alpha)(l_{t-1} + b_{t-1}) \\
    &\text{Уравнение тренда}\quad & b_t & =\beta (l_t - l_{t-1}) +(1-\beta) b_{t-1} \\
    &\text{Уравнение сезонности}\quad & s_t & =\gamma \frac{y_t}{(l_{t-1} + b_{t-1})} +(1-\gamma) s_{t-m}
  \end{array}
\end{equation}

Мультипликативные модели сезонности следует применять в следующих случаях:

\begin{enumerate}
  \item \textbf{Пропорциональные сезонные колебания:} Когда амплитуда сезонных колебаний увеличивается или уменьшается пропорционально уровню ряда. Например, если в высокий сезон продажи увеличиваются на 50\% от текущего уровня, а в низкий сезон уменьшаются на 30\% от текущего уровня.

    \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.9\linewidth]{images/03_ETS_01_weekly_proportional_seasonality.png}
      \caption{Недельные данные с пропорциональной сезонностью}
      \label{fig:ets_proportional_seasonality}
    \end{figure}

  \item \textbf{Рост с увеличивающейся волатильностью и относительные изменения:} Для временных рядов с трендом, где размах колебаний увеличивается со временем, и когда сезонные эффекты выражены в относительных величинах. Например, данные о продажах компании, которая растет год от года с увеличивающейся волатильностью, или рост продаж на 20\% в декабре по сравнению с ноябрем каждый год.

    \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.9\linewidth]{images/03_ETS_05_relative_changes.png}
      \caption{Относительные изменения сезонных эффектов: рост продаж на 20\% в декабре}
      \label{fig:ets_relative_changes}
    \end{figure}
\end{enumerate}

Несмотря на свою полезность, мультипликативные модели имеют ряд ограничений:

\begin{enumerate}
  \item \textbf{Нулевые и отрицательные значения:} Мультипликативные модели не работают с рядами, содержащими нулевые или отрицательные значения, так как деление на ноль невозможно, а умножение на отрицательные значения может привести к неинтерпретируемым результатам.

  \item \textbf{Сложность интерпретации:} Мультипликативные компоненты сложнее интерпретировать по сравнению с аддитивными, особенно при анализе вклада каждой компоненты в общее значение.

  \item \textbf{Чувствительность к выбросам:} Мультипликативные модели могут быть более чувствительны к выбросам, так как экстремальные значения могут значительно влиять на оценку сезонных коэффициентов.
\end{enumerate}

На рисунке \ref{fig:ets_exp_smoothing_models} представлены комбинации моделей экспоненциального сглаживания в рамках ETS-подхода. Модели с мультипликативным трендом также существуют в литературе, однако они менее популярны на практике из-за неустойчивости и сложности интерпретации.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{images/03_ETS_03_exponential_smoothing_models.png}
  \caption{Модели экспоненциального сглаживания в рамках ETS. Источник: \cite{hyndman2021}}
  \label{fig:ets_exp_smoothing_models}
\end{figure}

\section{ETS}

Все рассмотренные до этого модели не были стохастическими. Для них можно было просто выписать функционал на основе MSE и оптимизировать градиентным спуском. Однако на все описанные модели путём нескольких простых переходов можно навесить стохастику.

Основной принцип перехода от детерминированных моделей к стохастическим ETS-моделям заключается во включении случайных компонентов в уравнения состояния. Последующие переходы к более сложным ETS-моделям (с трендом, сезонностью и т.д.) следуют этому же принципу, но требуют более громоздкого математического оформления. Конкретный переход от модели \eqref{eq:deterministic_ets} к стохастической модели \eqref{eq:stochastic_ets} иллюстрирует общий подход к таким преобразованиям. Приведём пример для самой простой версии экспоненциального сглаживания:

\begin{equation}
  \begin{array}{llll}
    &\text{Уравнение прогноза} \quad &   \hat{y}_{t+1|t} & = l_t \\
    &\text{Уравнение сглаживания}\quad & l_t & =\alpha y_t +(1-\alpha) l_{t-1}
  \end{array}
  \label{eq:deterministic_ets}
\end{equation}

Вспомним, что уравнение сглаживания можно переписать в форме error correction:

\begin{equation}
  l_t = \alpha y_t +(1-\alpha) l_{t-1} =  l_{t-1}  + \alpha (y_t - l_{t-1}) =  l_{t-1}  + \alpha e_t,
\end{equation}

где $e_t = y_t - l_{t-1} = y_t - \hat{y}_{t|t-1}$

Исходя из определения $e_t$:

\begin{equation}
  y_t = l_{t-1} + e_t
\end{equation}

Достаточно естественно в данной постановке моделировать $e_t$ через распределение. Например, $e_t = \varepsilon_t = \mathcal{N}(0, \sigma^2)$.

Таким образом, итоговая модель:

\begin{equation}
  \begin{array}{llll}
    &\text{Уравнение наблюдения} \quad &   y_t & = l_{t-1} + \varepsilon_t \\
    &\text{Уравнение состояния}\quad & l_t & = l_{t-1}  + \alpha \varepsilon_t,
  \end{array}
  \label{eq:stochastic_ets}
\end{equation}

\subsection{Модели пространства состояний}

Модель пространства состояний (state space model) — это общий математический подход для описания динамических систем, в котором система представляется через набор скрытых (ненаблюдаемых) переменных, называемых \textit{состояниями}. Эти состояния эволюционируют во времени по некоторым законам, а наблюдаемые переменные связаны с состояниями через наблюдательные уравнения.

Общая форма модели пространства состояний состоит из двух уравнений:

\begin{equation}
  \begin{split}
    &\text{Уравнение состояния (state equation):} \quad   \alpha_{t} = F_t(\alpha_{t-1}, \eta_t) \\
    &\text{Наблюдательное уравнение (observation equation):} \quad y_t = G_t(\alpha_t, \varepsilon_t)
  \end{split}
\end{equation}

где $\alpha_t$ — вектор скрытых состояний в момент времени $t$, $F_t$ и $G_t$ — функции перехода состояний и наблюдений соответственно, $\eta_t$ и $\varepsilon_t$ — шумы процесса и наблюдений.

Модели ETS являются частным случаем моделей пространства состояний. В ETS-моделях скрытые состояния соответствуют компонентам временного ряда: уровню ($l_t$), тренду ($b_t$) и сезонности ($s_t$). Эти состояния эволюционируют во времени по определенным законам сглаживания, а наблюдаемые значения временного ряда связаны с состояниями через аддитивную или мультипликативную структуру.

\subsection{Правдоподобие ETS}

Для оценки параметров ETS необходимо выписать функцию правдоподобия. Рассмотрим совместную плотность распределения всех наблюдений $y_1, y_2, \ldots, y_T$. На первый взгляд может показаться, что можно воспользоваться стандартной формулой для совместной плотности:

\begin{equation}
  p(y_1, y_2, \ldots, y_T) = \prod_{t=1}^{T} p(y_t)
\end{equation}

Однако эта формула применима только для \textit{независимых} случайных величин. В случае временных рядов это предположение неверно, поскольку каждое наблюдение $y_t$ зависит от предыдущих через скрытые состояния модели $l_{t-1}, b_{t-1}$.

Более того, в ETS-моделях скрытые состояния сами зависят от всей истории наблюдений. Например, уровень $l_t$ определяется через предыдущий уровень $l_{t-1}$ и предыдущий тренд $b_{t-1}$, а также шум $\varepsilon_t$. Это создает сложную структуру зависимостей, при которой совместная плотность не может быть тривиально представлена как произведение маргинальных плотностей.

Для построения правдоподобия необходимо учитывать эти зависимости. Слегка преобразуем совместную плотность через формулу условных вероятностей:

\begin{equation}
  p(y_t | y_1, y_2, \ldots, y_{t-1}) = \frac{p(y_1, y_2, \ldots, y_t)}{p(y_1, y_2, \ldots, y_{t-1})}.
\end{equation}

Отсюда следует:

\begin{equation}
  p(y_1, y_2, \ldots, y_t) = p(y_t | y_1, y_2, \ldots, y_{t-1}) \cdot p(y_1, y_2, \ldots, y_{t-1})
\end{equation}

Применяя это соотношение последовательно, получаем:

\begin{equation}
  p(y_1, y_2, \ldots, y_T) = p(y_1) \prod_{t=2}^{T} p(y_t | y_1, y_2, \ldots, y_{t-1})
\end{equation}

В ETS-моделях скрытые состояния $l_t, b_t$ содержат в себе всю необходимую информацию о прошлом, чтобы сделать прогноз на следующий момент времени. Другими словами, при заданных скрытых состояниях $l_{t-1}, b_{t-1}$, наблюдение $y_t$ условно независимо от всей предыдущей истории наблюдений $y_1, y_2, \ldots, y_{t-1}$. Формально это записывается как:

\begin{equation}
  p(y_t | y_1, y_2, \ldots, y_{t-1}, l_{t-1}, b_{t-1}, \theta) = p(y_t | l_{t-1}, b_{t-1}, \theta)
\end{equation}

Это означает, что вся информация о прошлом, содержащаяся в последовательности наблюдений $y_1, y_2, \ldots, y_{t-1}$, уже учтена в текущих значениях скрытых состояний $l_{t-1}, b_{t-1}$. Скрытые состояния играют роль достаточной статистики для предсказания будущих наблюдений.

Условная плотность наблюдений при заданных начальных состояниях имеет вид:

\begin{equation}
  p(y_1, y_2, \ldots, y_T | l_0, b_0) = \prod_{t=1}^{T} p(y_t | l_{t-1}, b_{t-1}, \theta)
\end{equation}

где $\theta$ — вектор параметров модели (в случае ETS(AAN): $\alpha, \beta, \sigma^2$).

Для модели ETS(AAN) (Additive trend, Additive noise) наблюдение $y_t$ зависит только от уровня $l_{t-1}$ и тренда $b_{t-1}$ предыдущего момента времени. В предположении нормальности шумов $\varepsilon_t \sim \mathcal{N}(0, \sigma^2)$, каждое условное распределение будет нормальным:

\begin{equation}
  y_t \mid l_{t-1}, b_{t-1} \sim \mathcal{N}(l_{t-1} + b_{t-1}, \sigma^2)
\end{equation}

Таким образом, для модели ETS(AAN) логарифмическая функция правдоподобия принимает вид:

\begin{equation}
  \log L(\alpha, \beta, \sigma^2) = -\frac{T}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^{T} (y_t - l_{t-1} - b_{t-1})^2
\end{equation}

Максимизация этой функции по параметрам $\alpha, \beta, \sigma^2$ эквивалентна минимизации суммы квадратов остатков $\sum_{t=1}^{T} (y_t - l_{t-1} - b_{t-1})^2$, что согласуется с подходом наименьших квадратов, использованным ранее.

\subsection{Мультипликативные модели}

Мультипликативные модели ETS используются в тех же случаях, что и соответствующие модели экспоненциального сглаживания с мультипликативной компонентой. Они применяются, когда амплитуда сезонных или трендовых колебаний пропорциональна уровню ряда.

\subsubsection{Мультипликативная сезонность}

Ниже выпишем для сравнения ETS-модель с мультипликативной сезонностью:

ETS(A,A,M) модель:
\begin{equation}
  \begin{array}{llll}
    &\text{Уравнение наблюдения} \quad &   y_t & = (l_{t-1} + b_{t-1}) s_{t-m} + \varepsilon_t \\
    &\text{Уравнение уровня}\quad & l_t & = l_{t-1} + b_{t-1} + \alpha \frac{\varepsilon_t}{s_{t-m}} \\
    &\text{Уравнение тренда}\quad & b_t & = b_{t-1} + \beta \frac{\varepsilon_t}{s_{t-m}} \\
    &\text{Уравнение сезонности}\quad & s_t & = s_{t-m} + \gamma \frac{\varepsilon_t}{l_{t-1} + b_{t-1}}
  \end{array}
\end{equation}

\subsubsection{Мультипликативная ошибка}

В рамках ETS-подхода особое место занимают модели с мультипликативной ошибкой. В отличие от моделей с аддитивной ошибкой, где ошибка прибавляется к прогнозу, в моделях с мультипликативной ошибкой ошибка умножается на прогноз.

Примером такой модели может служить ETS(M,A,A):

\begin{equation}
  \begin{array}{llll}
    &\text{Уравнение наблюдения} \quad &   y_t & = (l_{t-1} + b_{t-1})(1 + \varepsilon_t) \\
    &\text{Уравнение уровня}\quad & l_t & = l_{t-1} + b_{t-1} + \alpha (l_{t-1} + b_{t-1}) \varepsilon_t \\
    &\text{Уравнение тренда}\quad & b_t & = b_{t-1} + \beta (l_{t-1} + b_{t-1}) \varepsilon_t
  \end{array}
\end{equation}

Модели с мультипликативной ошибкой применяются в следующих случаях:

\begin{enumerate}
  \item \textbf{Пропорциональная изменчивость:} Когда дисперсия ошибки пропорциональна уровню ряда. Это означает, что при больших значениях ряда ошибки также становятся больше в абсолютном выражении.

  \item \textbf{Относительные ошибки:} Когда более естественно рассматривать ошибки в относительных, а не абсолютных величинах. Например, ошибка в 10 единиц для ряда со средним уровнем 1000 и ошибка в 100 единиц для ряда со средним уровнем 10000 могут быть сопоставимы в относительных терминах.

  \item \textbf{Строгая положительность:} Когда временной ряд строго положителен, так как мультипликативная ошибка может привести к отрицательным значениям при больших отрицательных ошибках.
\end{enumerate}

Однако, модели с мультипликативной ошибкой имеют ряд ограничений. Во-первых, такие модели могут быть численно нестабильны, особенно когда значения временного ряда близки к нулю. Во-вторых, эти модели применимы только к строго положительным временным рядам, так как умножение на $(1 + \varepsilon_t)$ может привести к отрицательным или нулевым наблюдаемым значениям, если $\varepsilon_t \leq -1$.

Следует отметить, что три комбинации могут привести к численным трудностям: ETS(A,N,M), ETS(A,A,M) и ETS(A,A$_d$,M), из-за деления на значения, потенциально близкие к нулю в уравнениях состояния. Эти комбинации обычно не рассматриваются при выборе модели.

Модели с мультипликативными ошибками полезны, когда данные строго положительны, но численно нестабильны, когда данные содержат нули или отрицательные значения. Поэтому модели с мультипликативной ошибкой не рассматриваются, если временной ряд не является строго положительным.

\subsection{Таксономия ETS-моделей}
На рисунке \ref{fig:ets_state_space_models} представлена структура моделей для различных комбинаций тренда, сезонности и ошибки.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{images/03_ETS_04_state_space_models.png}
  \caption{Модели пространства состояний в рамках ETS. Источник: \cite{hyndman2021}}
  \label{fig:ets_state_space_models}
\end{figure}

\newpage
\begin{thebibliography}{1}
  \bibitem{hyndman2021}
  Hyndman, R.J., \& Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on January 25, 2026.
\end{thebibliography}

\end{document}